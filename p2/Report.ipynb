{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 2: Continuous control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.pseudo-code ul { list-style-type: none;}</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import HTML\n",
    "style = \"<style>.pseudo-code ul { list-style-type: none;}</style>\"\n",
    "HTML(style)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1: Learning algorithm\n",
    "\n",
    "### Description of algorithm\n",
    "\n",
    "<style>\n",
    ".pseudo-code ul {\n",
    "  list-style-type: none;\n",
    "}\n",
    ".pseudo-code ol {\n",
    "  list-style-type: none;\n",
    "}\n",
    "</style>\n",
    "\n",
    "Deep learning and reinforcement learning have recently been combined in different ways, including variants of \"Deep Q Network\" (DQN) as used in the Navigation project. While DQN works well with high-dimensional _observation spaces_, it can only handle discrete and low dimensional _action spaces_.\n",
    "\n",
    "In physical control tasks such as this one, we have continuous and high dimensional action spaces and algothithms like _deep deterministic policy gradient_ (DDPG) are better suited. In brief, DDPG is a model-free, off-policy actor-critic algorithm using deep function approximators.\n",
    "\n",
    "The DDPG algorithm works as follows:\n",
    "\n",
    "<nav class=\"pseudo-code\">\n",
    "\n",
    "* Randomly initialize critic network $Q(s,a|\\theta^Q)$ and actor $\\mu(s|\\theta^\\mu)$ with weights $\\theta^Q$ and $\\theta^\\mu.$\n",
    "* Initialize target network $Q'$ and $\\mu'$ with weights $\\theta^{Q'} \\leftarrow \\theta^Q, \\theta^{\\mu'} \\leftarrow \\theta^\\mu$\n",
    "* Initialize replay buffer $R$\n",
    "   \n",
    "   **for** episode = 1, M **do**\n",
    "   \n",
    "   * Initialize a random process $\\mathcal{N}$ for action exploration\n",
    "\n",
    "   * Receive initial observation state $s_1$\n",
    "\n",
    "   * **for** t = 1, T **do**\n",
    "     * Select action $a_t = \\mu(s_t|\\theta^\\mu) + \\mathcal{N}_t$ according to the current policy and exploration noise\n",
    "   \n",
    "     * Execute action $a_t$ and observe reward $r_t$ and observe new state $s_{t+1}$\n",
    "   \n",
    "     * Store transition $(s_t, a_t, r_t, s_{t+1})$ in $R$\n",
    "   \n",
    "     * Sample a random minibatch of $\\mathcal{N}$ transitions $(s_i, a_i, r_i, s_{i+1})$ from $R$\n",
    "   \n",
    "     * Set $y_i = r_i + \\gamma Q'(s_{i+1}, \\mu'(s_{i+1}|\\theta^{\\mu'})|\\theta^{Q'})$\n",
    "   \n",
    "     * Update cretic by minimizing the loss: $L = \\frac{1}{N} \\sum_i(y_i - Q(s_i, a_i|\\theta^Q))^2$\n",
    "   \n",
    "     * Update the actor policy using the sampled policy gradient:\n",
    "   \n",
    "     * \\begin{equation} \\nabla_{\\theta^\\mu} J \\approx \\frac{1}{N} \\sum_i \\nabla_a Q(s,a|\\theta^Q)|_{s=s_i, a=\\mu(s_i)} \\nabla_{\\theta^\\mu}\\mu(s|\\theta^\\mu)|_{s_i}  \\end{equation}\n",
    "   \n",
    "     * Update the target networks:\n",
    "   \n",
    "   \\begin{align}\n",
    "   \\theta^{Q'} &\\leftarrow \\tau\\theta^Q + (t-\\tau) \\theta^{Q'}\\\\\n",
    "   \\theta^{\\mu'} &\\leftarrow \\tau\\theta^\\mu + (t-\\tau)\\theta^{\\mu'}\n",
    "   \\end{align}\n",
    "\n",
    "   * **end for**\n",
    "\n",
    "  **end for**\n",
    "\n",
    "</nav>\n",
    "\n",
    "\n",
    "### Chosen Hyperparameters\n",
    "\n",
    "```\n",
    "BUFFER_SIZE = int(1e5)  # replay buffer size\n",
    "BATCH_SIZE = 128        # minibatch size\n",
    "GAMMA = 0.99            # discount factor\n",
    "TAU = 1e-3              # for soft update of target parameters\n",
    "LR_ACTOR = 2e-4         # learning rate of the actor \n",
    "LR_CRITIC = 3e-4        # learning rate of the critic\n",
    "WEIGHT_DECAY = 0        # L2 weight decay\n",
    "NUM_AGENTS = 20         # number of agents\n",
    "UPDATE_RATE = 20        # number of time steps between updates\n",
    "NUM_UPDATES = 10        # how many times train the agens on each update\n",
    "EPSILON = .5            # initial noise magnitude\n",
    "EPSILON_DECAY = 0.01    # noise decay per episode\n",
    "```\n",
    "\n",
    "### Neural Networks\n",
    "\n",
    "Two similarly structured networks are used for the _actor_ and the _critic_. For this problem ``state_size = 33``\n",
    " and ``action_size = 4``.\n",
    "\n",
    "#### Actor\n",
    "A basic three-layered feed-forward network with fully connected layers is used for the actor:\n",
    "\n",
    "* BatchNorm 1\n",
    "* Layer 1: (state_size, 128)\n",
    "* ReLU 1\n",
    "* BatchNorm 2\n",
    "* Layer 2: (128, 128)\n",
    "* ReLU 2\n",
    "* BatchNorm 3\n",
    "* Layer 3: (128, action_size)\n",
    "* Tanh\n",
    "\n",
    "#### Critic\n",
    "A slightly wider three-layered feed-forward network with fully connected layers is used for the critic:\n",
    "\n",
    "* Layer 1: (state_size, 256)\n",
    "* ReLU 1\n",
    "* BatchNorm\n",
    "* Layer 2: (cat(128, action_size), 128)\n",
    "* ReLU 2\n",
    "* Layer 3: (128, action_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2: Plot of Rewards\n",
    "\n",
    "This setup was able to solve the problem in 50 episodes. A plot of score per episode is illustraded below:\n",
    "\n",
    "<img src=\"score-plot.png\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3: Ideas for Future Work\n",
    "\n",
    "While the problem is solved reasonably quickly with basic Deep Q-learning, the agent could possibly improved in ways including:   \n",
    "\n",
    "* Search for optimal hyperparameters and neural network size/shape.\n",
    "* Try extensions of the Q-learning algorithm, including \"Double DQN\" and \"Dueling DQN\", and apply prioritized experience replay rather than the current uniform implementation.\n",
    "* Observe the raw pixels instead (or in addition to) the current ray-based \"sensor\" inputs. \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drlnd",
   "language": "python",
   "name": "drlnd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
