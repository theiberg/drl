{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 2: Continuous control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.pseudo-code ul { list-style-type: none;}</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import HTML\n",
    "style = \"<style>.pseudo-code ul { list-style-type: none;}</style>\"\n",
    "HTML(style)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1: Learning algorithm\n",
    "\n",
    "### Description of algorithm\n",
    "\n",
    "Generally, in Q-learning, the goal is to find the true underlying Q-values, represented by $q^*$. In Deep Q-learning, neural networks are used to approximate $q^*$. The objective is then to find the weights, $w$, of the Q-network which best approximates $q^*$. That is, to minimize\n",
    "\n",
    "\\begin{equation} \\min_w (q^*(s,a) - q(s,a;w))^2  \\end{equation}\n",
    "\n",
    "The Double Q-Network model with an Experience Buffer and Fixed Q-targets can be summarized as follows:\n",
    "\n",
    "1. We start by creating a Replay Buffer (for Experience Replay) of size BUFFER_SIZE. This means that we will save the last BUFFER_SIZE experiences consisting of ($s_t$, $a_t$, $r_t$, $s_{t+1}$, $d_t$) tuples where \n",
    "    * $s_t$ is the state at step t,\n",
    "    * $a_t$ is the action chosen at step t,\n",
    "    * $r_t$ is the reward received after taking action $a_t$ at state $s_t$,\n",
    "    * $s_{t+1}$ is the resulting next state, and\n",
    "    * $d_t$ which is True if we ended up in a terminal state.\n",
    "    \n",
    "The motivation for the Replay Buffer is to break the correleation between consecutive (state, action)-pairs. By sampling randomly from this \"memory\", we break this correlation.\n",
    "\n",
    "2. Next, two neural networks are initialized with random weights. The network $q(s,a;w)$ is used to approximate the true underlying Q-values. Therefore, if we only use one network, we would end up with the update \n",
    "\n",
    "\\begin{equation} \\Delta w = \\alpha(r + \\gamma \\max_a q(s',a;w) - q(s,a;w))\\nabla_w q(s,a;w)  , \\end{equation}\n",
    "\n",
    "which turns out to be unstable, since the weights we are updating are also present in the TD-target (i.e., $r + \\gamma \\max_a q(s',a;w)$). Therefore, we introduce \"fixed Q-targets\". This means that during learning, we fix the target, $w'$, and get\n",
    "\n",
    "\\begin{equation} \\Delta w = \\alpha(r + \\gamma \\max_a q(s',a;w') - q(s,a;w))\\nabla_w q(s,a;w). \\end{equation}\n",
    "\n",
    "3. After initializations, we proceed iteratively:\n",
    "    * From the given state $s_t$, choose action using an $\\epsilon$-greedy strategy and observe reward $r_t$, the next state $s_{t+1}$ and boolean $d_t$ to see if the episode has ended.\n",
    "    * Store the experience ($s_t$, $a_t$, $r_t$, $s_{t+1}$, $d_t$) in the Replay Buffer.\n",
    "    * If ``t % UPDATE_EVERY = 0``, we draw a sample of BATCH_SIZE from the Replay Buffer, and take an optimization step according to the equation above.\n",
    "    * Update the fixed Q-targets after learning, by the soft update \n",
    "        \\begin{equation} w' = \\tau w + (1-\\tau) w'. \\end{equation}\n",
    "    This process continues until convergence. In this particular case, the problem is considered solved when the agent obtains an average score above 13 over 100 consecutive episodes.\n",
    "\n",
    "\n",
    "<style>\n",
    ".pseudo-code ul {\n",
    "  list-style-type: none;\n",
    "}\n",
    "</style>\n",
    "\n",
    "* The DDPG algorithm can be summarized as follows:\n",
    "\n",
    "<nav class=\"pseudo-code\">\n",
    "\n",
    "```\n",
    "\n",
    "Randomly initialize critic network $Q(s,a|\\theta^Q)$ and actor $\\mu(s|\\theta^\\mu)$ with weights $\\theta^Q$ and $\\theta^\\mu.$\n",
    "\n",
    "Initialize target network $Q'$ and $\\mu'$ with weights $\\theta^{Q'} \\leftarrow \\theta^Q, \\theta^{\\mu'} \\leftarrow \\theta^\\mu$\n",
    "\n",
    "Initialize replay buffer $R$\n",
    "\n",
    "**for** episode = 1, M **do**\n",
    "\n",
    "  Initialize a random process $\\mathcal{N}$ for action exploration\n",
    "  \n",
    "  Receive initial observation state $s_1$\n",
    "  \n",
    "  **for** t = 1, T **do**\n",
    "  \n",
    "   Select action $a_t = \\mu(s_t|\\theta^\\mu) + \\mathcal{N}_t$ according to the current policy and exploration noise\n",
    "   \n",
    "   Execute action $a_t$ and observe reward $r_t$ and observe new state $s_{t+1}$\n",
    "   \n",
    "   Store transition $(s_t, a_t, r_t, s_{t+1})$ in $R$\n",
    "   \n",
    "   Sample a random minibatch of $\\mathcal{N}$ transitions $(s_i, a_i, r_i, s_{i+1})$ from $R$\n",
    "   \n",
    "   Set $y_i = r_i + \\gamma Q'(s_{i+1}, \\mu'(s_{i+1}|\\theta^{\\mu'})|\\theta^{Q'})$\n",
    "   \n",
    "   Update cretic by minimizing the loss: $L = \\frac{1}{N} \\sum_i(y_i - Q(s_i, a_i|\\theta^Q))^2$\n",
    "   \n",
    "   Update the actor policy using the sampled policy gradient:\n",
    "   \n",
    "   \\begin{equation} \\nabla_{\\theta^\\mu} J \\approx \\frac{1}{N} \\sum_i \\nabla_a Q(s,a|\\theta^Q)|_{s=s_i, a=\\mu(s_i)} \\nabla_{\\theta^\\mu}\\mu(s|\\theta^\\mu)|_{s_i}  \\end{equation}\n",
    "   \n",
    "   Update the target networks:\n",
    "   \n",
    "   \\begin{align}\n",
    "   \\theta^{Q'} &\\leftarrow \\tau\\theta^Q + (t-\\tau) \\theta^{Q'}\\\\\n",
    "   \\theta^{\\mu'} &\\leftarrow \\tau\\theta^\\mu + (t-\\tau)\\theta^{\\mu'}\n",
    "   \\end{align}\n",
    "\n",
    "  **end for**\n",
    "\n",
    "**end for**\n",
    "\n",
    "```\n",
    "\n",
    "</nav>\n",
    "\n",
    "  \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Chosen Hyperparameters\n",
    "\n",
    "* ``BUFFER_SIZE = int(1e5)``:  Replay buffer size\n",
    "* ``BATCH_SIZE = 64``:         Minibatch size \n",
    "* ``GAMMA = 0.99``:            Discount factor\n",
    "* ``TAU = 1e-3``:              For soft update of target parameters\n",
    "* ``LR = 5e-4``:               Learning rate\n",
    "* ``UPDATE_EVERY = 4``:        How often to update the network\n",
    "\n",
    "\n",
    "### Neural Network\n",
    "\n",
    "The neural network used is a simple feed-forward network with fully connected layers:\n",
    "\n",
    "* Layer 1: (state_size, 64)\n",
    "* ReLU 1\n",
    "* Layer 2: (64, 64)\n",
    "* ReLU 2\n",
    "* Layer 3: (64, action_size)\n",
    "\n",
    "Here, state_size = 37 and action_size = 4."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "header-includes:\n",
    "  - \\usepackage[ruled,vlined,linesnumbered]{algorithm2e}\n",
    "---\n",
    "\n",
    "# Algorithm 1\n",
    "Just a sample algorithmn\n",
    "\\begin{algorithm}[H]\n",
    "\\DontPrintSemicolon\n",
    "\\SetAlgoLined\n",
    "\\KwResult{Write here the result}\n",
    "\\SetKwInOut{Input}{Input}\\SetKwInOut{Output}{Output}\n",
    "\\Input{Write here the input}\n",
    "\\Output{Write here the output}\n",
    "\\BlankLine\n",
    "\\While{While condition}{\n",
    "    instructions\\;\n",
    "    \\eIf{condition}{\n",
    "        instructions1\\;\n",
    "        instructions2\\;\n",
    "    }{\n",
    "        instructions3\\;\n",
    "    }\n",
    "}\n",
    "\\caption{While loop with If/Else condition}\n",
    "\\end{algorithm} "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2: Plot of Rewards\n",
    "\n",
    "This setup was able to solve the problem in 462 episodes. A plot of score per episode is illustraded below:\n",
    "\n",
    "<img src=\"score-plot.png\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3: Ideas for Future Work\n",
    "\n",
    "While the problem is solved reasonably quickly with basic Deep Q-learning, the agent could possibly improved in ways including:   \n",
    "\n",
    "* Search for optimal hyperparameters and neural network size/shape.\n",
    "* Try extensions of the Q-learning algorithm, including \"Double DQN\" and \"Dueling DQN\", and apply prioritized experience replay rather than the current uniform implementation.\n",
    "* Observe the raw pixels instead (or in addition to) the current ray-based \"sensor\" inputs. \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drlnd",
   "language": "python",
   "name": "drlnd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
