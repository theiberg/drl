{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 1: Navigation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1: Learning algorithm\n",
    "\n",
    "### Description of algorithm\n",
    "\n",
    "Generally, in Q-learning, the goal is to find the true underlying Q-values, represented by $q^*$. In Deep Q-learning, neural networks are used to approximate $q^*$. The objective is then to find the weights, $w$, of the Q-network which best approximates $q^*$. That is, to minimize\n",
    "\n",
    "\\begin{equation} \\min_w (q^*(s,a) - q(s,a;w))^2  \\end{equation}\n",
    "\n",
    "The Double Q-Network model with an Experience Buffer and Fixed Q-targets can be summarized as follows:\n",
    "\n",
    "1. We start by creating a Replay Buffer (for Experience Replay) of size BUFFER_SIZE. This means that we will save the last BUFFER_SIZE experiences consisting of ($s_t$, $a_t$, $r_t$, $s_{t+1}$, $d_t$) tuples where \n",
    "    * $s_t$ is the state at step t,\n",
    "    * $a_t$ is the action chosen at step t,\n",
    "    * $r_t$ is the reward received after taking action $a_t$ at state $s_t$,\n",
    "    * $s_{t+1}$ is the resulting next state, and\n",
    "    * $d_t$ which is True if we ended up in a terminal state.\n",
    "    \n",
    "The motivation for the Replay Buffer is to break the correleation between consecutive (state, action)-pairs. By sampling randomly from this \"memory\", we break this correlation.\n",
    "\n",
    "2. Next, two neural networks are initialized with random weights. The network $q(s,a;w)$ is used to approximate the true underlying Q-values. Therefore, if we only use one network, we would end up with the update \n",
    "\n",
    "\\begin{equation} \\Delta w = \\alpha(r + \\gamma \\max_a q(s',a;w) - q(s,a;w))\\nabla_w q(s,a;w)  , \\end{equation}\n",
    "\n",
    "which turns out to be unstable, since the weights we are updating are also present in the TD-target (i.e., $r + \\gamma \\max_a q(s',a;w)$). Therefore, we introduce \"fixed Q-targets\". This means that during learning, we fix the target, $w'$, and get\n",
    "\n",
    "\\begin{equation} \\Delta w = \\alpha(r + \\gamma \\max_a q(s',a;w') - q(s,a;w))\\nabla_w q(s,a;w). \\end{equation}\n",
    "\n",
    "3. After initializations, we proceed iteratively:\n",
    "    * From the given state $s_t$, choose action using an $\\epsilon$-greedy strategy and observe reward $r_t$, the next state $s_{t+1}$ and boolean $d_t$ to see if the episode has ended.\n",
    "    * Store the experience ($s_t$, $a_t$, $r_t$, $s_{t+1}$, $d_t$) in the Replay Buffer.\n",
    "    * If ``t % UPDATE_EVERY = 0``, we draw a sample of BATCH_SIZE from the Replay Buffer, and take an optimization step according to the equation above.\n",
    "    * Update the fixed Q-targets after learning, by the soft update \n",
    "        \\begin{equation} w' = \\tau w + (1-\\tau) w'. \\end{equation}\n",
    "    This process continues until convergence. In this particular case, the problem is considered solved when the agent obtains an average score above 13 over 100 consecutive episodes.\n",
    "\n",
    "### Chosen Hyperparameters\n",
    "\n",
    "* ``BUFFER_SIZE = int(1e5)``:  Replay buffer size\n",
    "* ``BATCH_SIZE = 64``:         Minibatch size \n",
    "* ``GAMMA = 0.99``:            Discount factor\n",
    "* ``TAU = 1e-3``:              For soft update of target parameters\n",
    "* ``LR = 5e-4``:               Learning rate\n",
    "* ``UPDATE_EVERY = 4``:        How often to update the network\n",
    "\n",
    "\n",
    "### Neural Network\n",
    "\n",
    "The neural network used is a simple feed-forward network with fully connected layers:\n",
    "\n",
    "* Layer 1: (state_size, 64)\n",
    "* ReLU 1\n",
    "* Layer 2: (64, 64)\n",
    "* ReLU 2\n",
    "* Layer 3: (64, action_size)\n",
    "\n",
    "Here, state_size = 37 and action_size = 4."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2: Plot of Rewards\n",
    "\n",
    "This setup was able to solve the problem in 462 episodes. A plot of score per episode is illustraded below:\n",
    "\n",
    "<img src=\"score-plot.png\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3: Ideas for Future Work\n",
    "\n",
    "While the problem is solved reasonably quickly with basic Deep Q-learning, the agent could possibly improved in ways including:   \n",
    "\n",
    "* Search for optimal hyperparameters and neural network size/shape.\n",
    "* Try extensions of the Q-learning algorithm, including \"Double DQN\" and \"Dueling DQN\", and apply prioritized experience replay rather than the current uniform implementation.\n",
    "* Observe the raw pixels instead (or in addition to) the current ray-based \"sensor\" inputs. \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drlnd",
   "language": "python",
   "name": "drlnd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
